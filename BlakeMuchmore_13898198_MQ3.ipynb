{"cells":[{"cell_type":"markdown","metadata":{"id":"r63rpM7Sff2b"},"source":["# Assignment 3: Reinforcement Learning\n","In this assignment you will apply the RL algorithms you learnt from the tutorials to a simulated robot car in a pybullet environment.\n","\n","You will be asked to (percentages are allocation of assignment marks):\n","* Train the robot to drive to the green goal marker which spawns at random locations (60%)\n","* Modify the epsilon-greedy function to incorporate prior knowledge (20%)\n","* Modify the reward function (10%)\n","* Add obstacles to the environment (10%)\n","\n","It is highly recommended to install pybullet and run your code locally since things will run much faster. It will also make editing the gym environment code easier.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sx8knrhyhteV"},"source":["## Simple Car Environment\n","---\n","\n","![simple_car.gif](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExODU0NmVlMzU1MGU1YzJjMjA5ODE5NjM0MTg0MTU1ZmM1OTA1NzRkNCZjdD1n/VI3OuvQShK3gzENiVz/giphy.gif)\n","\n","*(code for this tutorial adapted from: https://gerardmaggiolino.medium.com/creating-openai-gym-environments-with-pybullet-part-2-a1441b9a4d8e*)\n","\n","---\n","\n","This is a simple car environment with a continuous state space and discrete action space with the goal of driving towards a green marker. Driving within 1.5 metres of the green marker causes the episode to end or if a certain amount of time has passed.\n","\n","We can instantiate the environment as follows:\n","\n","\n","```\n","env = gym.make('SimpleDriving-v0', apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='tp_camera')\n","```\n","\n","\n","### Action Space\n","\n","*   0: Reverse-Left\n","*   1: Reverse\n","*   2: Reverse-Right\n","*   3: Steer-Left (no throttle)\n","*   4: No throttle and no steering\n","*   5: Steer-Right (no throttle)\n","*   6: Forward-right\n","*   7: Forward\n","*   8: Forward-left\n","\n","### Observation Space\n","Two dimensional array with distance in (x, y) from goal position.\n","\n","###Rewards\n","Negative euclidean distance from the goal.\n","\n","### Interacting with the Environment\n","We can sample actions randomly, get the agent to perform that action and then observe how the environment state changes:\n","```\n","state = env.reset()  # this needs to be called once at the start before sending any actions\n","action = env.action_space.sample()\n","state, reward, done, _, info = env.step(action)\n","```\n","\n","\n","---\n","\n","## Installing and Modifying Gym Environment Code\n","\n","For installing in collab you would have already been familiar with using the following command:\n","```\n","pip install git+https://github.com/fredsukkar/simple-car-env-template\n","```\n","\n","To edit the gym environment first create a github account and then go to https://github.com/fredsukkar/simple-car-env-template and create a new repository using the repository as a template as follows:\n","![sdlfk](https://i.ibb.co/MMsLv1G/github-template.jpg)\n","\n","\n","Once you have your own copy of the repository you can then edit the files in the browser via github or alternatively (recommended) you can [clone the repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) and modify the code locally.\n","\n","To install the package from github you can use the usual:\n","```\n","pip install git+https://github.com/<your repo>\n","```\n","\n","To install the package locally:\n","```\n","cd /path/to/cloned_repo\n","python setup.py install\n","```\n","\n","Note that for both methods you will need to install the package again after you've made any changes for them to take any effect.\n","\n","The main file you will be modifying is: https://github.com/fredsukkar/Gym-Medium-Post/blob/main/simple_driving/envs/simple_driving_env.py.\n","\n","There are four main functions that you have been calling via the gym environment object:\n","```\n","class SimpleDrivingEnv(gym.Env):\n","    metadata = {'render.modes': ['human']}  \n","  \n","    def __init__(self):\n","        pass\n","\n","    def step(self, action):\n","        pass\n","\n","    def reset(self):\n","        pass\n","\n","    def render(self):\n","        pass\n","```\n","\n","Parts 3 and 4 of the assignment will ask you to modify one of these functions.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"b1j7Dqubpfql"},"source":["Before we can execute any code we first need to install the following packages:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OzqrVWfmZIqa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n","E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"]}],"source":["%pip install gym==0.26.2 pyvirtualdisplay pygame torch > /dev/null 2>&1\n","%pip install git+https://github.com/BlakeMuchmore01/AIIR_MQ3_Car_Env.git > /dev/null 2>&1\n","# %pip install git+https://github.com/fredsukkar/simple-car-env-template > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!apt-get install -y xvfb"]},{"cell_type":"markdown","metadata":{"id":"WqiZQ4hUp3lv"},"source":["Now import the necessary packages and following helper functions (you don't need the `display_video` function if running locally):"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3xeRJtf7p_q1"},"outputs":[],"source":["import os\n","os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0' \n","\n","import gym\n","import simple_driving\n","# import pybullet_envs\n","import pybullet as p\n","import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display\n","from IPython.display import HTML\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import numpy as np\n","import math\n","from collections import defaultdict\n","import pickle\n","from IPython.display import clear_output\n","import torch\n","import random\n","\n","display = Display(visible=0, size=(400, 300))\n","display.start()\n","\n","def display_video(frames, framerate=30):\n","  \"\"\"Generates video from `frames`.\n","\n","  Args:\n","    frames (ndarray): Array of shape (n_frames, height, width, 3).\n","    framerate (int): Frame rate in units of Hz.\n","\n","  Returns:\n","    Display object.\n","  \"\"\"\n","  height, width, _ = frames[0].shape\n","  dpi = 70\n","  orig_backend = matplotlib.get_backend()\n","  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n","  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n","  matplotlib.use(orig_backend)  # Switch back to the original backend.\n","  ax.set_axis_off()\n","  ax.set_aspect('equal')\n","  ax.set_position([0, 0, 1, 1])\n","  im = ax.imshow(frames[0])\n","  def update(frame):\n","    im.set_data(frame)\n","    return [im]\n","  interval = 1000/framerate\n","  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n","                                  interval=interval, blit=True, repeat=False)\n","  return HTML(anim.to_html5_video())"]},{"cell_type":"markdown","metadata":{"id":"ps9E66nS-Cr7"},"source":["## Part 1\n","\n","Train the robot to drive to the green goal marker. Use any of the RL algorithms you learnt in the tutorials.\n","\n","You can save the model after training to save you having to retrain everytime you open colab:\n","```\n","from google.colab import drive\n","drive.mount('/content/drive')\n","torch.save(model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/simple_driving_qlearning.pkl\")  # this will save to folder \"Colab Notebooks\" on your google drive\n","```\n","\n","You can then load the model:\n","```\n","model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/simple_driving_qlearning.pkl\"))\n","```\n","\n","Once loaded you can even continue your training again using the learnt weights, that way you don't have to start from scratch again if you decide you want to train for longer. But keep in mind the epsilon greedy function will start from completely random again so you might want to set epsilon starting value appropriately."]},{"cell_type":"markdown","metadata":{"id":"xXODTRa7_WAz"},"source":["## Part 2\n","\n","Incorporate prior knowledge into the epsilon-greedy function by choosing a non-uniform distribution to sample from when performing exploration. For example, for training flappy bird we used the following to sample flapping actions less often to avoid flying off into the sky during early stages of training:\n","\n","```\n","return np.random.choice(np.array(range(2)), p=[0.9,0.1])\n","```\n","\n","Note that you will need to change the parameters to suit the car's action space and also choose a suitable distribution."]},{"cell_type":"markdown","metadata":{"id":"XasnJTZ2Bynb"},"source":["## Part 3\n","\n","Modify the reward to give a bonus of 50 if the goal is reached. You can do this either in the `simulate` function or directly by modifying the `step` function in the gym environment code.\n"]},{"cell_type":"markdown","metadata":{"id":"ztxrzwvMCfnj"},"source":["## Part 4\n","\n","Add obstacles to the environment. You can do this by modifying the `reset` function in the gym environment code. For example you can add objects as follows:\n","```\n","self.obstacle = self._p.loadURDF(fileName=<path to urdf file here>,\n","                   basePosition=[0, 0, 0])\n","```\n","\n","An example urdf file: https://github.com/fredsukkar/simple-car-env-template/blob/main/simple_driving/resources/simplegoal.urdf\n","\n","**Note:** you will need to add features to your state so that the agent learns to avoid obstacles. For example, you could add the x, y distance from the agent to the closest obstacle in the environment. Then your state would become: `[x_goal, y_goal, x_obstacle, y_obstacle]`.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"MqdyWkJWB40I"},"source":["Here is some code to help you get started."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["## Hyperparameters\n","EPISODES = 2500                  # Number of episodes to run for training\n","LEARNING_RATE = 0.00025          # Learning rate for optimizing neural network weights\n","MEM_SIZE = 50000                 # Maximum size of replay memory\n","REPLAY_START_SIZE = 10000        # Amount of samples to fill replay memory before training\n","BATCH_SIZE = 32                  # Number of samples to draw from replay memory for training\n","GAMMA = 0.99                     # Discount factor for future rewards\n","EPSILON_START = 0.1              # Starting exploration rate\n","EPSILON_END = 0.0001             # Final exploration rate\n","EPSILON_DECAY = 4 * MEM_SIZE     # Decay rate for exploration rate\n","MEM_RETRAIN = 0.1                # Percentage of memory to retrain on each episode\n","NETWORK_UPDATE_ITERS = 5000      # Number of steps to update target network\n","\n","FC1_DIMS = 128                   # Number of neurons in our MLP's first hidden layer\n","FC2_DIMS = 128                   # Number of neurons in our MLP's second hidden layer\n","\n","# Metrics for displaying training status\n","best_reward = 0\n","average_reward = 0\n","episode_history = []\n","episode_reward_history = []\n","np.bool = np.bool_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Neural network class to approximate Q-values\n","class Network(torch.nn.Module):\n","    def __init__(self, env):\n","        super().__init__()\n","        self.input_shape = env.observation_space.shape\n","        self.action_space = env.action_space.n\n","\n","        # build an MLP with 2 hidden layers\n","        self.layers = torch.nn.Sequential(\n","            torch.nn.Linear(*self.input_shape, FC1_DIMS),   # input layer\n","            torch.nn.ReLU(),     # this is called an activation function\n","            torch.nn.Linear(FC1_DIMS, FC2_DIMS),    # hidden layer\n","            torch.nn.ReLU(),     # this is called an activation function\n","            torch.nn.Linear(FC2_DIMS, self.action_space)    # output layer\n","            )\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n","        self.loss = nn.MSELoss()  # loss function\n","\n","    def forward(self, x):\n","        return self.layers(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Replay Buffer class for storing and retrieving sampled experiences\n","class ReplayBuffer:\n","    def __init__(self, env):\n","        # Initialising memory count and creating arrays to store experiences\n","        self.mem_count = 0\n","        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n","        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n","        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n","        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n","        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n","\n","    def add(self, state, action, reward, state_, done):\n","        # If memory count is at max size, overwrite previous values\n","        if self.mem_count < MEM_SIZE:\n","            mem_index = self.mem_count\n","        else:\n","            # Avoiding catastrophic forgetting - retrain initial 10% of the replay buffer\n","            mem_index = ...\n","\n","        self.states[mem_index]  = state     # Storing the state\n","        self.actions[mem_index] = action    # Storing the action\n","        self.rewards[mem_index] = reward    # Storing the reward\n","        self.states_[mem_index] = state_    # Storing the next state\n","        self.dones[mem_index] =  1 - done   # Storing the done flag\n","        self.mem_count += 1                 # Incrementing memory count\n","    \n","    def sample(self):\n","        # Randomly sample a batch of experiences\n","        MEM_MAX = min(self.mem_count, MEM_SIZE)\n","        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n","\n","        states  = self.states[batch_indices]    # Getting the states\n","        actions = self.actions[batch_indices]   # Getting the actions\n","        rewards = self.rewards[batch_indices]   # Getting the rewards\n","        states_ = self.states_[batch_indices]   # Getting the next states\n","        dones   = self.dones[batch_indices]     # Getting the done flags\n","\n","        # Returning the random sampled experiences\n","        return states, actions, rewards, states_, dones"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DQN_Solver:\n","    def __init__(self, env):\n","        self.memory = ReplayBuffer(env)     # Creating a replay buffer\n","        self.policy_network = Network(env)  # Q\n","        self.target_network = Network(env)  # \\hat{Q}\n","        self.target_network.load_state_dict(self.policy_network.state_dict())  # Initially set weights of Q to \\hat{Q}\n","        self.learn_count = 0    # keep track of the number of iterations we have learnt for\n","\n","    # Epsilon-greedy policy\n","    def choose_action(self, observation):\n","        # Only start decaying epsilon once we start learning (once replay memory has REPLAY_START_SIZE samples)\n","        if self.memory.mem_count > REPLAY_START_SIZE:\n","            eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n","                math.exp(-1. * self.learn_count / EPSILON_DECAY)\n","        else:\n","            eps_threshold = 1.0\n","\n","        # If we rolled a value lower than the epsilon sample a random action\n","        if random.random() < eps_threshold:\n","            return ...\n","        \n","        # Otherwise policy network (Q) chooses action with highest estimated Q value so far\n","        state = torch.tensor(observation).float().detach()\n","        state = state.unsqueeze(0)\n","        self.policy_network.eval()\n","        with torch.no_grad():\n","            q_values = ...  # Get Q-values from policy network\n","\n","        return torch.argmax(q_values).item()\n","    \n","    # Main training loop\n","    def learn(self):\n","        states, actions, rewards, states_, dones = self.memory.sample()  # Sample a batch of random experiences\n","        states = torch.tensor(states, dtype=torch.float32)      # Convert states to tensor\n","        actions = torch.tensor(actions, dtype=torch.long)       # Convert actions to tensor\n","        rewards = torch.tensor(rewards, dtype=torch.float32)    # Convert rewards to tensor\n","        states_ = torch.tensor(states_, dtype=torch.float32)    # Convert next states to tensor\n","        dones = torch.tensor(dones, dtype=torch.bool)           # Convert done flags to tensor\n","        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)   # Create an array of batch indices\n","\n","        self.policy_network.train(True)\n","        q_values = self.policy_network(states)  # Get Q-value estimates from policy network\n","        q_values = q_values[batch_indices, actions]  # Get Q-values for the sampled actions\n","\n","        self.target_network.eval()\n","        with torch.no_grad():\n","            q_values_next = ...  # Get Q-values of states_ from target nework (Q_hat)\n","\n","        q_values_next_max = torch.max(q_values_next, dim=1)[0]  # Max Q-values for next state\n","        q_target = rewards + GAMMA * q_values_next_max * dones  # Calculate target Q-values\n","\n","        loss = self.policy_network.loss(..., ...)  # Calculate loss from target Q-values and predicted Q-values\n","\n","        # Compute gradients and update Q weights\n","        self.policy_network.optimizer.zero_grad()\n","        loss.backward()\n","        self.policy_network.optimizer.step()  # Update Q weights\n","        self.learn_count += 1  # Increment learn count\n","\n","        # Set target network weights to policy networks wights every C steps\n","        if self.learn_count % NETWORK_UPDATE_ITERS == NETWORK_UPDATE_ITERS - 1:\n","            print(\"Updating target network\")\n","            self.update_target_network()\n","        \n","    def update_target_network(self):\n","        self.target_network.load_state_dict(self.policy_network.state_dict())\n","\n","    def returning_epsilon(self):\n","        return self.exploration_rate"]},{"cell_type":"markdown","metadata":{},"source":["## Training Simple Car"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aifCJ2wKDJn"},"outputs":[],"source":["######################### renders image from third person perspective for validating policy ##############################\n","env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='tp_camera')\n","##########################################################################################################################\n","\n","######################### renders image from onboard camera ###############################################################\n","# env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='fp_camera')\n","##########################################################################################################################\n","\n","######################### if running locally you can just render the environment in pybullet's GUI #######################\n","# env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=True, isDiscrete=True)\n","##########################################################################################################################\n","\n","env.action_space.seed(0)    # Set default seed for reproducibility\n","random.seed(0)              # ...\n","np.random.seed(0)           # ...\n","torch.manual_seed(0)        # ...\n","episode_batch_score = 0     # Initialize episode score\n","episode_reward = 0          # Initialize episode reward\n","agent = DQN_Solver(env)     # Create DQN agent\n","plt.clf()                   # Clear previous plot\n","\n","for episode in range(EPISODES):\n","    state = env.reset()     # Reset environment\n","\n","    while True:\n","        # Sampling random actions and add to replay buffer\n","        action = agent.choose_action(state)\n","        state_, reward, done, info = env.step(action)\n","        reward += -abs(state[1])\n","        if done == True:\n","            reward += -50\n","\n","        agent.memory.add(state, action, reward, state_, done)  # Add experience to replay buffer\n","\n","        # Only start learning once replay memory has REPLAY_START_SIZE samples\n","        if agent.memory.mem_count > REPLAY_START_SIZE:\n","            agent.learn()\n","\n","        state = state_  # Update state\n","        episode_batch_score += reward  # Update episode score\n","        episode_reward += reward  # Update episode reward\n","\n","        if done:\n","            break\n","\n","    episode_history.append(episode)\n","    episode_reward_history.append(episode_reward)\n","    episode_reward = 0  # Reset episode reward\n","\n","    # Saving model every batches of 100 episodes\n","    if episode % 100 == 0 and agent.memory.mem_count > REPLAY_START_SIZE:\n","        save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n","        torch.save(agent.policy_network.state_dict(), save_path)\n","        print(\"average total reward per episode batch since episode \", episode, \": \", episode_batch_score/ float(100))\n","        episode_batch_score = 0\n","    elif agent.memory.mem_count < REPLAY_START_SIZE:\n","        print(\"waiting for buffer to fill...\")\n","        episode_batch_score = 0\n","\n","plt.plot(episode_history, episode_reward_history)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing Simple Car"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["######################### renders image from third person perspective for validating policy ##############################\n","env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='tp_camera')\n","##########################################################################################################################\n","\n","######################### renders image from onboard camera ###############################################################\n","# env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='fp_camera')\n","##########################################################################################################################\n","\n","######################### if running locally you can just render the environment in pybullet's GUI #######################\n","# env = gym.make(\"SimpleDriving-v0\", apply_api_compatibility=True, renders=True, isDiscrete=True)\n","##########################################################################################################################\n","\n","plt.plot(episode_history, episode_reward_history)\n","plt.show()\n","\n","agent = DQN_Solver(env)\n","agent.policy_network.load_state_dict(torch.load(\"policy_network.pkl\"))\n","state, info = env.reset()\n","frames = []\n","frames.append(env.render())\n","agent.policy_network.eval()\n","\n","while True:\n","    with torch.no_grad():\n","        q_values = agent.policy_network(torch.tensor(state, dtype=torch.float32))\n","\n","    action = torch.argmax(q_values).item()  # Selecting action with highest pred q-value\n","    state, reward, done, info = env.step(action)\n","    frames.append(env.render())\n","    if done:\n","        break\n","\n","env.close()\n","display_video(frames, framerate=5)  # remove if runnning locally"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
